# NHANES Data Manager - Streamlit Application
# –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–∞–±–æ—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö NHANES

import streamlit as st
import pandas as pd
import requests
import io
import os
from datetime import datetime
import zipfile
from pathlib import Path
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
st.set_page_config(
    page_title="NHANES Data Manager",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

class NHANESDataManager:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã–º–∏ NHANES"""

    def __init__(self):
        self.base_url = "https://ftp.cdc.gov/pub/health_statistics/nchs/nhanes/continuousnhanes"
        self.data_dir = Path("nhanes_data")
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö NHANES
        self.cycles = {
            "continuous": {
                "2021-2023": {"name": "NHANES 08/2021-08/2023", "suffix": "L"},
                "2017-2020": {"name": "NHANES 2017-March 2020", "suffix": "P"},
                "2017-2018": {"name": "NHANES 2017-2018", "suffix": "J"},
                "2015-2016": {"name": "NHANES 2015-2016", "suffix": "I"},
                "2013-2014": {"name": "NHANES 2013-2014", "suffix": "H"},
                "2011-2012": {"name": "NHANES 2011-2012", "suffix": "G"},
                "2009-2010": {"name": "NHANES 2009-2010", "suffix": "F"},
                "2007-2008": {"name": "NHANES 2007-2008", "suffix": "E"},
                "2005-2006": {"name": "NHANES 2005-2006", "suffix": "D"},
                "2003-2004": {"name": "NHANES 2003-2004", "suffix": "C"},
                "2001-2002": {"name": "NHANES 2001-2002", "suffix": "B"},
                "1999-2000": {"name": "NHANES 1999-2000", "suffix": ""}
            },
            "historical": {
                "NHANES III": "NHANES III (1988-1994)",
                "NHANES II": "NHANES II (1976-1980)",
                "NHANES I": "NHANES I (1971-1975)",
                "Hispanic HANES": "Hispanic HANES (1982-1984)"
            }
        }

        self.categories = {
            "DEMO": {
                "name": "Demographics",
                "description": "–î–µ–º–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ: –≤–æ–∑—Ä–∞—Å—Ç, –ø–æ–ª, —Ä–∞—Å–∞/—ç—Ç–Ω–∏—á–Ω–æ—Å—Ç—å, –¥–æ—Ö–æ–¥—ã",
                "color": "#1f77b4",
                "files": ["DEMO"]
            },
            "DIET": {
                "name": "Dietary",
                "description": "–î–∞–Ω–Ω—ã–µ –æ –ø–∏—Ç–∞–Ω–∏–∏: –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∏—â–∏, –Ω—É—Ç—Ä–∏–µ–Ω—Ç—ã, –ø–∏—â–µ–≤—ã–µ –¥–æ–±–∞–≤–∫–∏",
                "color": "#ff7f0e",
                "files": ["DR1TOT", "DR2TOT", "DR1IFF", "DR2IFF", "DRXFCD", "DBQ"]
            },
            "EXAM": {
                "name": "Examination",
                "description": "–§–∏–∑–∏—á–µ—Å–∫–∏–µ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: –∞–Ω—Ç—Ä–æ–ø–æ–º–µ—Ç—Ä–∏—è, –∞—Ä—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–µ –¥–∞–≤–ª–µ–Ω–∏–µ, —Ç–µ—Å—Ç—ã",
                "color": "#2ca02c",
                "files": ["BMX", "BPX", "BPQ", "AUQ", "AUX", "CVX"]
            },
            "LAB": {
                "name": "Laboratory",
                "description": "–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã–µ –∞–Ω–∞–ª–∏–∑—ã: –±–∏–æ—Ö–∏–º–∏—è –∫—Ä–æ–≤–∏, —Ç–æ–∫—Å–∏–∫–∞–Ω—Ç—ã, –±–∏–æ–º–∞—Ä–∫–µ—Ä—ã",
                "color": "#d62728",
                "files": ["CBC", "HDL", "TRIGLY", "TCHOL", "ALB_CR", "CRP"]
            },
            "Q": {
                "name": "Questionnaire",
                "description": "–û–ø—Ä–æ—Å–Ω–∏–∫–∏: –∑–¥–æ—Ä–æ–≤—å–µ, –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –∏—Å—Ç–æ—Ä–∏—è, –æ–±—Ä–∞–∑ –∂–∏–∑–Ω–∏",
                "color": "#9467bd",
                "files": ["HSQ", "DIQ", "ALQ", "SMQ", "ACQ", "CDQ"]
            }
        }

    def get_xpt_url(self, cycle, file_prefix):
        """–ü–æ–ª—É—á–∏—Ç—å URL –¥–ª—è XPT —Ñ–∞–π–ª–∞"""
        if cycle == "1999-2000":
            suffix = ""
        else:
            suffix = "_" + self.cycles["continuous"][cycle]["suffix"]

        file_name = f"{file_prefix}{suffix}.XPT"
        year_path = cycle.replace("-", "-")
        return f"{self.base_url}/{year_path}/{file_name}"

    def download_xpt_file(self, url, local_path):
        """–°–∫–∞—á–∞—Ç—å XPT —Ñ–∞–π–ª"""
        try:
            response = requests.get(url, stream=True, timeout=30)
            response.raise_for_status()

            # Check if the response is HTML (error page)
            content_type = response.headers.get('content-type', '').lower()
            if 'text/html' in content_type:
                logger.error(f"URL {url} returned HTML instead of XPT file")
                return False

            with open(local_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)

            # Validate that the downloaded file is actually an XPT file
            if not self._is_valid_xpt_file(local_path):
                logger.error(f"Downloaded file {local_path} is not a valid XPT file")
                # Remove the invalid file
                if local_path.exists():
                    local_path.unlink()
                return False

            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {url}: {e}")
            return False

    def _is_valid_xpt_file(self, file_path):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –≤–∞–ª–∏–¥–Ω—ã–º XPT —Ñ–∞–π–ª–æ–º"""
        try:
            with open(file_path, 'rb') as f:
                # Read first few bytes to check XPT header
                header = f.read(80)
                # XPT files start with specific header bytes
                # Check for SAS XPORT format signature
                if len(header) < 80:
                    return False
                # XPT files have "SAS" in the header
                return b'SAS' in header[:10] or b'XPORT' in header[:10]
        except Exception:
            return False

    def xpt_to_dataframe(self, xpt_path):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å XPT —Ñ–∞–π–ª –≤ DataFrame"""
        try:
            # Try pandas read_sas first
            df = pd.read_sas(xpt_path, format='xport')
            return df
        except Exception as e1:
            logger.warning(f"Pandas read_sas failed for {xpt_path}: {e1}")
            try:
                # Try alternative: pyreadstat
                import pyreadstat
                df, meta = pyreadstat.read_xport(str(xpt_path))
                logger.info(f"Successfully read {xpt_path} with pyreadstat")
                return df
            except Exception as e2:
                logger.warning(f"Pyreadstat failed for {xpt_path}: {e2}")
                try:
                    # Try alternative: sas7bdat
                    from sas7bdat import SAS7BDAT
                    with SAS7BDAT(str(xpt_path)) as f:
                        df = f.to_data_frame()
                    logger.info(f"Successfully read {xpt_path} with sas7bdat")
                    return df
                except Exception as e3:
                    logger.error(f"All methods failed for {xpt_path}: pandas({e1}), pyreadstat({e2}), sas7bdat({e3})")
                    return None

    def download_all_data(self, progress_callback=None):
        """–°–∫–∞—á–∞—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ NHANES"""
        total_files = 0
        downloaded_files = 0

        # –ü–æ–¥—Å—á–∏—Ç–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤
        for cycle in self.cycles["continuous"]:
            for category, cat_info in self.categories.items():
                total_files += len(cat_info["files"])

        for cycle_key, cycle_info in self.cycles["continuous"].items():
            cycle_dir = self.data_dir / cycle_key
            cycle_dir.mkdir(parents=True, exist_ok=True)

            for category, cat_info in self.categories.items():
                category_dir = cycle_dir / category.lower()
                category_dir.mkdir(parents=True, exist_ok=True)

                for file_prefix in cat_info["files"]:
                    url = self.get_xpt_url(cycle_key, file_prefix)
                    local_path = category_dir / f"{file_prefix}_{cycle_key.replace('-', '_')}.xpt"

                    if not local_path.exists():
                        success = self.download_xpt_file(url, local_path)
                        if success:
                            downloaded_files += 1
                        else:
                            logger.warning(f"Failed to download {file_prefix} from {url}")

                        if progress_callback:
                            status_msg = f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ: {file_prefix} ({cycle_key})"
                            if not success:
                                status_msg += " - –û–®–ò–ë–ö–ê"
                            progress = downloaded_files / total_files
                            progress_callback(progress, status_msg)

        return downloaded_files, total_files

    def get_available_datasets(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
        datasets = []

        for cycle_key in self.cycles["continuous"]:
            cycle_dir = self.data_dir / cycle_key
            if cycle_dir.exists():
                for category in self.categories:
                    category_dir = cycle_dir / category.lower()
                    if category_dir.exists():
                        for xpt_file in category_dir.glob("*.xpt"):
                            datasets.append({
                                "cycle": cycle_key,
                                "category": category,
                                "file": xpt_file.name,
                                "path": xpt_file,
                                "size": xpt_file.stat().st_size
                            })

        return datasets

    def load_dataset_as_csv(self, dataset_path):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ CSV"""
        try:
            df = self.xpt_to_dataframe(dataset_path)
            if df is not None:
                csv_buffer = io.StringIO()
                df.to_csv(csv_buffer, index=False, encoding='utf-8')
                return csv_buffer.getvalue()
            return None
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ CSV: {e}")
            return None

@st.cache_data
def load_nhanes_manager():
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–µ–Ω–µ–¥–∂–µ—Ä –¥–∞–Ω–Ω—ã—Ö NHANES"""
    return NHANESDataManager()

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    st.title("üìä NHANES Data Manager")
    st.markdown("**–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö National Health and Nutrition Examination Survey**")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
    manager = load_nhanes_manager()

    # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å
    st.sidebar.title("–ù–∞–≤–∏–≥–∞—Ü–∏—è")
    page = st.sidebar.selectbox(
        "–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É",
        ["–û–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö", "–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö", "–≠–∫—Å–ø–æ—Ä—Ç –≤ CSV", "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"]
    )

    if page == "–û–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö":
        show_data_overview(manager)
    elif page == "–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö":
        show_download_page(manager)
    elif page == "–≠–∫—Å–ø–æ—Ä—Ç –≤ CSV":
        show_export_page(manager)
    elif page == "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞":
        show_statistics_page(manager)

def show_data_overview(manager):
    """–ü–æ–∫–∞–∑–∞—Ç—å –æ–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö NHANES"""
    st.header("üìã –û–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö NHANES")

    st.markdown("""
    **NHANES (National Health and Nutrition Examination Survey)** ‚Äî —ç—Ç–æ –ø—Ä–æ–≥—Ä–∞–º–º–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π,
    –ø—Ä–æ–≤–æ–¥–∏–º—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∑–¥–æ—Ä–æ–≤—å—è –∏ –ø–∏—Ç–∞–Ω–∏—è –≤–∑—Ä–æ—Å–ª—ã—Ö –∏ –¥–µ—Ç–µ–π –≤ –°–æ–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –®—Ç–∞—Ç–∞—Ö.
    """)

    # –¶–∏–∫–ª—ã –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
    st.subheader("üóìÔ∏è –¶–∏–∫–ª—ã –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è (1999-2023)**")
        for cycle, info in manager.cycles["continuous"].items():
            st.markdown(f"‚Ä¢ **{cycle}**: {info['name']}")

    with col2:
        st.markdown("**–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è**")
        for cycle, name in manager.cycles["historical"].items():
            st.markdown(f"‚Ä¢ **{cycle}**: {name}")

    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–∞–Ω–Ω—ã—Ö
    st.subheader("üìä –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–∞–Ω–Ω—ã—Ö")

    for category, info in manager.categories.items():
        with st.container():
            col1, col2 = st.columns([1, 4])
            with col1:
                st.markdown(f"**{category}**", unsafe_allow_html=True)
            with col2:
                st.markdown(f"**{info['name']}** - {info['description']}")
                st.markdown(f"*–û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã: {', '.join(info['files'])[:50]}...*")
            st.markdown("---")

def show_download_page(manager):
    """–°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö"""
    st.header("‚¨áÔ∏è –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö NHANES")

    st.markdown("""
    –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –Ω–∏–∂–µ, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö NHANES.
    –î–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –ª–æ–∫–∞–ª—å–Ω–æ –∏ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω—ã –ø–æ —Ü–∏–∫–ª–∞–º –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º.
    """)

    st.warning("‚ö†Ô∏è **–í–∞–∂–Ω–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ:** –ü—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ XPT —Ñ–∞–π–ª—ã NHANES –±–æ–ª—å—à–µ –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç. –î–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ—â–µ–Ω—ã –Ω–∞ –¥—Ä—É–≥–∏–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã.")

    # –ü—Ä–æ–≤–µ—Ä–∏–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Å–∫–∞—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    datasets = manager.get_available_datasets()

    if datasets:
        st.success(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(datasets)} —Å–∫–∞—á–∞–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö")

        if st.button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ"):
            with st.spinner("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö..."):
                progress_bar = st.progress(0)
                status_text = st.empty()

                def update_progress(progress, message):
                    progress_bar.progress(progress)
                    status_text.text(message)

                downloaded, total = manager.download_all_data(update_progress)

                if downloaded > 0:
                    st.success(f"‚úÖ –°–∫–∞—á–∞–Ω–æ {downloaded} –∏–∑ {total} —Ñ–∞–π–ª–æ–≤")
                else:
                    st.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–µ—Ä–æ–≤ NHANES.")
                    st.info("üí° **–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞:** –î–∞–Ω–Ω—ã–µ NHANES —Ç–∞–∫–∂–µ –¥–æ—Å—Ç—É–ø–Ω—ã —á–µ—Ä–µ–∑ IPUMS NHANES: https://nhanes.ipums.org/")
    else:
        st.warning("‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ù–∞—á–Ω–∏—Ç–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ.")

        if st.button("üì• –ù–∞—á–∞—Ç—å —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö"):
            with st.spinner("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö NHANES..."):
                progress_bar = st.progress(0)
                status_text = st.empty()

                def update_progress(progress, message):
                    progress_bar.progress(progress)
                    status_text.text(message)

                downloaded, total = manager.download_all_data(update_progress)

                if downloaded > 0:
                    st.success(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–æ {downloaded} –∏–∑ {total} —Ñ–∞–π–ª–æ–≤")
                    st.rerun()
                else:
                    st.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö. –ü—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ XPT —Ñ–∞–π–ª—ã –±–æ–ª—å—à–µ –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã.")
                    st.info("üí° **NHANES –¥–∞–Ω–Ω—ã–µ —Ç–µ–ø–µ—Ä—å –¥–æ—Å—Ç—É–ø–Ω—ã —á–µ—Ä–µ–∑ —Å–ª–µ–¥—É—é—â–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:**")
                    st.markdown("**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö NHANES:**")
                    st.markdown("- [IPUMS NHANES](https://nhanes.ipums.org/) - –≥–∞—Ä–º–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ CSV —Ñ–æ—Ä–º–∞—Ç–µ")
                    st.markdown("- [CDC Data Portal](https://data.cdc.gov/) - –ø–æ–∏—Å–∫ –ø–æ 'NHANES'")
                    st.markdown("- [NHANES Website](https://www.cdc.gov/nchs/nhanes/) - –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç —Å –¥–∞–Ω–Ω—ã–º–∏")
                    st.markdown("- [CDC FTP](https://ftp.cdc.gov/pub/health_statistics/nchs/nhanes/) - FTP —Å–µ—Ä–≤–µ—Ä (–º–æ–∂–µ—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å –ø—Ä–æ–≤–µ—Ä–∫–∏)")

def show_export_page(manager):
    """–°—Ç—Ä–∞–Ω–∏—Ü–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ CSV"""
    st.header("üìÅ –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –≤ CSV")

    datasets = manager.get_available_datasets()

    if not datasets:
        st.warning("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ —Å–∫–∞—á–∞–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ '–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö'")
        return

    # –§–∏–ª—å—Ç—Ä—ã
    st.subheader("üîç –§–∏–ª—å—Ç—Ä—ã")

    col1, col2 = st.columns(2)

    with col1:
        cycles = sorted(list(set([d["cycle"] for d in datasets])))
        selected_cycles = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ —Ü–∏–∫–ª—ã", cycles, default=cycles[:3])

    with col2:
        categories = sorted(list(set([d["category"] for d in datasets])))
        selected_categories = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏", categories, default=categories)

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    filtered_datasets = [
        d for d in datasets
        if d["cycle"] in selected_cycles and d["category"] in selected_categories
    ]

    if not filtered_datasets:
        st.warning("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤")
        return

    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    st.subheader(f"üìä –î–æ—Å—Ç—É–ø–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö ({len(filtered_datasets)})")

    df_info = pd.DataFrame([
        {
            "–¶–∏–∫–ª": d["cycle"],
            "–ö–∞—Ç–µ–≥–æ—Ä–∏—è": d["category"],
            "–§–∞–π–ª": d["file"],
            "–†–∞–∑–º–µ—Ä (MB)": round(d["size"] / 1024 / 1024, 2)
        } for d in filtered_datasets
    ])

    st.dataframe(df_info, use_container_width=True)

    # –ö–Ω–æ–ø–∫–∏ —ç–∫—Å–ø–æ—Ä—Ç–∞
    st.subheader("üíæ –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö")

    col1, col2, col3 = st.columns(3)

    with col1:
        if st.button("üìÅ –≠–∫—Å–ø–æ—Ä—Ç –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (ZIP)"):
            with st.spinner("–°–æ–∑–¥–∞–Ω–∏–µ ZIP –∞—Ä—Ö–∏–≤–∞..."):
                zip_buffer = create_zip_export(manager, filtered_datasets)
                if zip_buffer:
                    st.download_button(
                        label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å ZIP –∞—Ä—Ö–∏–≤",
                        data=zip_buffer.getvalue(),
                        file_name=f"nhanes_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
                        mime="application/zip"
                    )

    with col2:
        if st.button("üìã –≠–∫—Å–ø–æ—Ä—Ç —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã"):
            csv_summary = df_info.to_csv(index=False, encoding='utf-8')
            st.download_button(
                label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å CSV —Å–≤–æ–¥–∫—É",
                data=csv_summary,
                file_name=f"nhanes_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )

    # –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç —Ñ–∞–π–ª–æ–≤
    if st.expander("üîß –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç —Ñ–∞–π–ª–æ–≤"):
        selected_dataset = st.selectbox(
            "–í—ã–±–µ—Ä–∏—Ç–µ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞",
            filtered_datasets,
            format_func=lambda x: f"{x['cycle']} - {x['category']} - {x['file']}"
        )

        if st.button("üì§ –≠–∫—Å–ø–æ—Ä—Ç –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"):
            with st.spinner(f"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è {selected_dataset['file']} –≤ CSV..."):
                csv_data = manager.load_dataset_as_csv(selected_dataset["path"])
                if csv_data:
                    st.download_button(
                        label="‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å CSV —Ñ–∞–π–ª",
                        data=csv_data,
                        file_name=f"{selected_dataset['file'].replace('.xpt', '')}.csv",
                        mime="text/csv"
                    )
                else:
                    st.error("‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞")

def create_zip_export(manager, datasets):
    """–°–æ–∑–¥–∞—Ç—å ZIP –∞—Ä—Ö–∏–≤ —Å –¥–∞–Ω–Ω—ã–º–∏ CSV"""
    try:
        zip_buffer = io.BytesIO()

        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            for dataset in datasets:
                csv_data = manager.load_dataset_as_csv(dataset["path"])
                if csv_data:
                    csv_filename = f"{dataset['cycle']}/{dataset['category']}/{dataset['file'].replace('.xpt', '.csv')}"
                    zip_file.writestr(csv_filename, csv_data)

        zip_buffer.seek(0)
        return zip_buffer

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è ZIP: {e}")
        return None

def show_statistics_page(manager):
    """–°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
    st.header("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö")

    datasets = manager.get_available_datasets()

    if not datasets:
        st.warning("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ —Å–∫–∞—á–∞–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏")
        return

    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    st.subheader("üìä –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤", len(datasets))

    with col2:
        total_size = sum([d["size"] for d in datasets]) / 1024 / 1024 / 1024
        st.metric("–û–±—â–∏–π —Ä–∞–∑–º–µ—Ä", f"{total_size:.2f} GB")

    with col3:
        unique_cycles = len(set([d["cycle"] for d in datasets]))
        st.metric("–¶–∏–∫–ª–æ–≤", unique_cycles)

    with col4:
        unique_categories = len(set([d["category"] for d in datasets]))
        st.metric("–ö–∞—Ç–µ–≥–æ—Ä–∏–π", unique_categories)

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ü–∏–∫–ª–∞–º
    st.subheader("üìÖ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ü–∏–∫–ª–∞–º")

    cycle_counts = pd.DataFrame(datasets).groupby("cycle").size().reset_index(name="count")
    st.bar_chart(cycle_counts.set_index("cycle"))

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    st.subheader("üóÇÔ∏è –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")

    category_counts = pd.DataFrame(datasets).groupby("category").size().reset_index(name="count")
    st.bar_chart(category_counts.set_index("category"))

    # –î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    st.subheader("üìã –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")

    df_stats = pd.DataFrame([
        {
            "–¶–∏–∫–ª": d["cycle"],
            "–ö–∞—Ç–µ–≥–æ—Ä–∏—è": d["category"],
            "–§–∞–π–ª": d["file"],
            "–†–∞–∑–º–µ—Ä (MB)": round(d["size"] / 1024 / 1024, 2)
        } for d in datasets
    ])

    st.dataframe(df_stats.sort_values(["–¶–∏–∫–ª", "–ö–∞—Ç–µ–≥–æ—Ä–∏—è"]), use_container_width=True)

if __name__ == "__main__":
    main()
